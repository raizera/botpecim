import os
import json
from dotenv import load_dotenv
import pdfplumber
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from langchain_community.vectorstores.faiss import FAISS
from langchain.chains.question_answering import load_qa_chain
from langchain.prompts import PromptTemplate
import streamlit as st
import google.generativeai as genai

# Carrega vari√°veis de ambiente do arquivo .env, que cont√©m informa√ß√µes sens√≠veis como chaves de API
load_dotenv()
GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')  # Obt√©m a chave da API do Google a partir das vari√°veis de ambiente

# Configura o cliente da API Google Generative AI usando a chave da API fornecida
genai.configure(api_key=GOOGLE_API_KEY)

# Define o caminho para o arquivo PDF que ser√° processado
pdf_path = "database-site-pecim.pdf"

# Sistema de cache para armazenar respostas anteriores
CACHE_FILE = "response_cache.json"

# Carrega o cache de respostas do arquivo (se existir)
try:
    with open(CACHE_FILE, 'r') as f:
        response_cache = json.load(f)
except FileNotFoundError:
    response_cache = {}

# Fun√ß√£o para salvar o cache no arquivo
def save_cache():
    with open(CACHE_FILE, 'w') as f:
        json.dump(response_cache, f)

# Fun√ß√£o para ler e extrair todo o texto de um arquivo PDF
import pdfplumber

def extract_text_from_pdf(pdf_file_path):
    extracted_text = ""
    with pdfplumber.open(pdf_file_path) as pdf:
        for page in pdf.pages:
            extracted_text += page.extract_text()
    return extracted_text

# Fun√ß√£o para dividir o texto extra√≠do em peda√ßos menores para processamento
def split_text_into_chunks(text, chunk_size=5000, chunk_overlap=100):
    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
    return splitter.split_text(text)

# Fun√ß√£o para criar e salvar um banco de vetores (index) para os peda√ßos de texto
def create_vector_store(chunks):
    embeddings = GoogleGenerativeAIEmbeddings(model="models/text-embedding-004")
    vector_store = FAISS.from_texts(chunks, embedding=embeddings)
    vector_store.save_local("faiss_index")

# Fun√ß√£o para configurar o modelo de conversa√ß√£o (chatbot) e o prompt para a cadeia de Pergunta/Resposta (QA)
def setup_qa_chain(language="en"):
    prompt_template = """
    Answer the question as detailed as possible from the provided context, make sure to provide all the details. If the answer is not in
    the provided context, just say, "answer is not available in the context", don't provide the wrong answer.\n\n
    Context:\n {context}\n
    Question: \n{question}\n
    Answer:
    """
    
    if language == "pt":
        prompt_template = """
        Responda √† pergunta da forma mais detalhada poss√≠vel com base no contexto fornecido, garantindo fornecer todos os detalhes. Se a resposta n√£o estiver no contexto fornecido, diga apenas: "a resposta n√£o est√° dispon√≠vel no contexto", n√£o forne√ßa a resposta errada.\n\n
        Contexto:\n {context}\n
        Pergunta: \n{question}\n
        Resposta:
        """
    
    model = ChatGoogleGenerativeAI(model="gemini-1.5-flash", client=genai, temperature=0.2)
    prompt = PromptTemplate(template=prompt_template, input_variables=["context", "question"])
    return load_qa_chain(llm=model, chain_type="stuff", prompt=prompt)

# Fun√ß√£o para limpar o hist√≥rico de mensagens do chat
def clear_chat_history():
    st.session_state.messages = [{"role": "assistant", "content": "O que voc√™ deseja saber sobre o PECIM?"}]

# Fun√ß√£o para processar a pergunta do usu√°rio e gerar uma resposta com base nos documentos dispon√≠veis
def generate_response(user_question, language="en"):
    if user_question in response_cache:
        return response_cache[user_question]

    embeddings = GoogleGenerativeAIEmbeddings(model="models/text-embedding-004")
    vector_store = FAISS.load_local("faiss_index", embeddings, allow_dangerous_deserialization=True)
    docs = vector_store.similarity_search(user_question)
    chain = setup_qa_chain(language=language)
    context = "\n".join([doc.page_content for doc in docs])

    while True:
        try:
            response = chain({"input_documents": docs, "context": context, "question": user_question}, return_only_outputs=True)
            output_text = response['output_text']

            response_cache[user_question] = output_text
            save_cache()

            return output_text

        except Exception:
            continue

# Fun√ß√£o principal para executar o aplicativo Streamlit
def main():
    st.set_page_config(page_title="PECIM's ChatBot", page_icon="üñê", layout="wide")

    if not os.path.exists("faiss_index"):
        with st.spinner("Processando o PDF..."):
            raw_text = extract_text_from_pdf(pdf_path)
            text_chunks = split_text_into_chunks(raw_text)
            create_vector_store(text_chunks)
            st.success("Processamento conclu√≠do com sucesso!")

    st.title("Bem-vindo(a) ao PECIM's ChatBot")
    st.markdown("Tire d√∫vidas e conhe√ßa o programa com aux√≠lio do Chatbot do [PECIM](https://www.pecim.unicamp.br/)")

    with st.sidebar:
        st.write("Esta aplica√ß√£o √© n√£o oficial e foi desenvolvida como teste por um estudante do PECIM.")
        st.write("Ela foi desenvolvida para responder apenas quest√µes sobre o PECIM. Qualquer pergunta de outra natureza n√£o ser√° respondida.")
        st.write("As respostas do chatbot s√£o baseadas nas informa√ß√µes contidas no site oficial do programa. Entretanto, as respostas do chat podem conter erros, aus√™ncia e falta de precis√£o durante a intera√ß√£o, principalmente sobre informa√ß√µes mais atualizadas e que n√£o estejam dispon√≠veis no site oficial.")
        st.write("Caso algum erro inesperado aconte√ßa (BUG), considere atualizar a p√°gina.")
        st.write("D√∫vidas e sugest√µes: r147725@dac.unicamp.br")
        
        # Adiciona um seletor de idioma
        language = st.selectbox("Selecione o idioma:", ("Portugu√™s", "Ingl√™s"))
        language_code = "pt" if language == "Portugu√™s" else "en"

        st.button('Limpar o hist√≥rico da conversa', on_click=clear_chat_history)

    if "messages" not in st.session_state:
        clear_chat_history()

    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(f"**{message['role'].capitalize()}:** {message['content']}")

    if user_prompt := st.chat_input():
        st.session_state.messages.append({"role": "user", "content": user_prompt})
        with st.chat_message("user"):
            st.markdown(f"**Voc√™:** {user_prompt}")

        if st.session_state.messages[-1]["role"] != "assistant":
            with st.chat_message("assistant"):
                with st.spinner("Pensando..."):
                    response = generate_response(user_prompt, language=language_code)
                    st.session_state.messages.append({"role": "assistant", "content": response})
                    st.markdown(f"**Chatbot:** {response}")

if __name__ == "__main__":
    main()
